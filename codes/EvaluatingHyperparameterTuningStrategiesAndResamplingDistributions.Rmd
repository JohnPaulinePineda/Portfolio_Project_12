---
title: 'Supervised Learning : Evaluating Hyperparameter Tuning Strategies and Resampling Distributions'
author: "<b><a href='https://github.com/JohnPaulinePineda'>John Pauline Pineda</a></b>"
date: "December 6, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This project implements different evaluation procedures for hyperparameter tuning strategies and resampling distributions using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>. Using **Support Vector Machine** and **Regularized Discriminant Analysis** model structures, methods applied in the analysis to implement hyperparameter tuning included the **Manual Grid Search**, **Automated Grid Search** and **Automated Random Search** with the hyperparameter selection process illustrated for each. The resulting predictions derived from the candidate models applying various hyperparameter tuning procedures were evaluated in terms of their discrimination power using the area under the receiver operating characteristics curve (AUROC) metric. All results were consolidated in a [<span style="color: #FF0000">**Summary**</span>](#summary) presented at the end of the document.
|
| Hyperparameter tuning is the process of determining the optimal combination of hyperparameter values that maximizes the model performance by running multiple trials in a single training process. Hyperparameters refer to external configuration variables that are used to control the learning process and have a significant effect on the performance of machine learning models but are fixed and cannot be directly learned from the regular training process. Machine learning models can have one to several hyperparameters and finding the best combination can be treated as a search problem. The search algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**caret**</mark> package) attempt to train the models sequentially by exploring various sets of hyperparameters to determine the best combination which optimizes model performance.
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**Sonar**</mark>  dataset from the  <mark style="background-color: #CCECFF">**mlbench**</mark> package was used for this illustrated example.
|
| Preliminary dataset assessment:
|
| **[A]** 208 rows (observations)
|      **[A.1]** Train Set = 167 observations
|      **[A.2]** Test Set = 41 observations
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
|     
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(mlbench)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR2)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)

##################################
# Loading source and
# formulating the train set
##################################
data(Sonar)
set.seed(12345678)
Sonar_Partition <- createDataPartition(Sonar$Class, p = .80, list = FALSE)
Sonar_Train <- Sonar[Sonar_Partition,]
Sonar_Test  <- Sonar[-Sonar_Partition,]

##################################
# Performing a general exploration of the train set
##################################
dim(Sonar_Train)
str(Sonar_Train)
summary(Sonar_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Sonar_Test)
str(Sonar_Test)
summary(Sonar_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Sonar_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

##  1.2 Data Quality Assessment
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 8 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[B.2]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[B.3]** <span style="color: #FF0000">V27</span> variable (numeric)
|      **[B.4]** <span style="color: #FF0000">V28</span> variable (numeric)
|      **[B.5]** <span style="color: #FF0000">V30</span> variable (numeric)
|      **[B.6]** <span style="color: #FF0000">V34</span> variable (numeric)
|      **[B.7]** <span style="color: #FF0000">V37</span> variable (numeric)
|      **[B.8]** <span style="color: #FF0000">V42</span> variable (numeric)
|
| **[C]** No low variance noted for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness noted for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Sonar_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,sapply(DQA.Predictors, is.numeric)]

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,sapply(DQA.Predictors, is.factor)]

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```

</details>

##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| **[A]** Outliers noted for 39 variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps.
|      **[A.1]** <span style="color: #FF0000">V1</span> variable (13 outliers detected)
|      **[A.2]** <span style="color: #FF0000">V2</span> variable (11 outliers detected)
|      **[A.3]** <span style="color: #FF0000">V3</span> variable (7 outliers detected)
|      **[A.4]** <span style="color: #FF0000">V4</span> variable (8 outliers detected)
|      **[A.5]** <span style="color: #FF0000">V5</span> variable (3 outliers detected)
|      **[A.6]** <span style="color: #FF0000">V6</span> variable (7 outliers detected)
|      **[A.7]** <span style="color: #FF0000">V7</span> variable (5 outliers detected)
|      **[A.8]** <span style="color: #FF0000">V8</span> variable (8 outliers detected)
|      **[A.9]** <span style="color: #FF0000">V9</span> variable (7 outliers detected)
|      **[A.10]** <span style="color: #FF0000">V10</span> variable (8 outliers detected)
|      **[A.11]** <span style="color: #FF0000">V11</span> variable (5 outliers detected)
|      **[A.12]** <span style="color: #FF0000">V12</span> variable (1 outliers detected)
|      **[A.13]** <span style="color: #FF0000">V13</span> variable (2 outliers detected)
|      **[A.14]** <span style="color: #FF0000">V14</span> variable (3 outliers detected)
|      **[A.15]** <span style="color: #FF0000">V15</span> variable (2 outliers detected)
|      **[A.16]** <span style="color: #FF0000">V24</span> variable (2 outliers detected)
|      **[A.17]** <span style="color: #FF0000">V38</span> variable (7 outliers detected)
|      **[A.18]** <span style="color: #FF0000">V39</span> variable (3 outliers detected)
|      **[A.19]** <span style="color: #FF0000">V40</span> variable (3 outliers detected)
|      **[A.20]** <span style="color: #FF0000">V41</span> variable (2 outliers detected)
|      **[A.21]** <span style="color: #FF0000">V42</span> variable (4 outliers detected)
|      **[A.22]** <span style="color: #FF0000">V43</span> variable (5 outliers detected)
|      **[A.23]** <span style="color: #FF0000">V44</span> variable (12 outliers detected)
|      **[A.24]** <span style="color: #FF0000">V45</span> variable (19 outliers detected)
|      **[A.25]** <span style="color: #FF0000">V46</span> variable (14 outliers detected)
|      **[A.26]** <span style="color: #FF0000">V47</span> variable (11 outliers detected)
|      **[A.27]** <span style="color: #FF0000">V48</span> variable (8 outliers detected)
|      **[A.28]** <span style="color: #FF0000">V49</span> variable (5 outliers detected)
|      **[A.29]** <span style="color: #FF0000">V50</span> variable (12 outliers detected)
|      **[A.30]** <span style="color: #FF0000">V51</span> variable (4 outliers detected)
|      **[A.31]** <span style="color: #FF0000">V52</span> variable (11 outliers detected)
|      **[A.32]** <span style="color: #FF0000">V53</span> variable (3 outliers detected)
|      **[A.33]** <span style="color: #FF0000">V54</span> variable (9 outliers detected)
|      **[A.34]** <span style="color: #FF0000">V55</span> variable (5 outliers detected)
|      **[A.35]** <span style="color: #FF0000">V56</span> variable (7 outliers detected)
|      **[A.36]** <span style="color: #FF0000">V57</span> variable (7 outliers detected)
|      **[A.37]** <span style="color: #FF0000">V58</span> variable (8 outliers detected)
|      **[A.38]** <span style="color: #FF0000">V59</span> variable (12 outliers detected)
|      **[A.39]** <span style="color: #FF0000">V60</span> variable (5 outliers detected)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```

</details>

###  1.3.2 Zero and Near-Zero Variance
|
| **[A]** Low variance noted for 8 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variables using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){

  print("No low variance predictors noted.")

} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))

  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))

  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))

  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }

  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLowVariance)
  
} 

```

</details>

###  1.3.3 Collinearity
|
| **[A]** No high correlation > 95% were noted for any variable as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  
  print("No highly correlated predictors noted.")
  
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))

  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05,
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))

}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedHighCorrelation)

}

```

</details>

###  1.3.4 Linear Dependencies
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))

  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }

}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)

  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLinearlyDependent)

} else {
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA)
  
}

```

</details>

###  1.3.5 Shape Transformation
|
| **[A]** A number of numeric variables in the dataset were observed to be right-skewed which required shape transformation for data distribution stability. Considering that all numeric variables were strictly positive values, the <span style="color: #0000FF">BoxCox</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was used to transform their distributional shapes.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Gathering descriptive statistics
##################################
(DPA_BoxCoxTransformedSkimmed <- skim(DPA_BoxCoxTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA_BoxCoxTransformed)

```

</details>

###  1.3.6 Centering and Scaling
|
| **[A]** To maintain numerical stability during modelling, centering and scaling transformations were applied on the transformed numeric variables. The <span style="color: #0000FF">center</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was implemented which subtracts the average value of a numeric variable to all the values. As a result of centering, the variables had zero mean values. In addition, the <span style="color: #0000FF">scale</span> method, also from the <mark style="background-color: #CCECFF">**caret**</mark> package, was applied which performs a center transformation with each value of the variable divided by its standard deviation. Scaling the data coerced the values to have a common standard deviation of one.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Applying a center and scale data transformation
##################################
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_BoxCoxTransformed, method = c("center","scale"))
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_BoxCoxTransformed)

##################################
# Gathering descriptive statistics
##################################
(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformedSkimmed <- skim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed)

```

</details>

###  1.3.7 Pre-Processed Dataset
|
| **[A]** 208 rows (observations)
|      **[A.1]** Train Set = 167 observations
|      **[A.2]** Test Set = 41 observations
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
| 
| **[C]** Pre-processing actions applied:
|      **[C.1]** Centering, scaling and shape transformation applied to improve data quality
|      **[C.2]** No outlier treatment applied since the high values noted were contextually valid and sensible 
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
| 
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
Class <- DPA$Class 
PMA.Predictors.Numeric  <- DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA.Predictors.Numeric)
PMA_PreModelling_Train <- PMA_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
DPA_Test <- Sonar_Test
DPA_Test.Predictors <- DPA_Test[,!names(DPA_Test) %in% c("Class")]
DPA_Test.Predictors.Numeric <- DPA_Test.Predictors[,sapply(DPA_Test.Predictors, is.numeric)]
DPA_Test_BoxCox <- preProcess(DPA_Test.Predictors.Numeric, method = c("BoxCox"))
DPA_Test_BoxCoxTransformed <- predict(DPA_Test_BoxCox, DPA_Test.Predictors.Numeric)
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_Test_BoxCoxTransformed, method = c("center","scale"))
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_Test_BoxCoxTransformed)

##################################
# Creating the pre-modelling
# test set
##################################
Class <- DPA_Test$Class 
PMA_Test.Predictors.Numeric  <- DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_Test_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA_Test.Predictors.Numeric)
PMA_PreModelling_Test <- PMA_Test_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```

</details>

## 1.4 Data Exploration
|
| **[A]** Numeric variables which demonstrated a differential relationships with the <span style="color: #FF0000">Class</span> response variable include:
|      **[A.1]** <span style="color: #FF0000">V6</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">V8</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">V13</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">V45</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">V50</span> variable (numeric)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric, 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|")

```

</details>

## 1.5 Predictive Model Development and Hyperparameter Tuning
###  1.5.1 Support Vector Machine - Radial Basis Function Kernel - Manual Grid Search (SVM_R_MGS)
|
| [Support Vector Machine](https://dl.acm.org/doi/10.1145/130385.130401) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| [Manual Grid Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the explicit selection of individual model hyperparameter values to be assessed as influenced by human domain background and experience. Based on these hyperparameter settings, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. While this process is practical in terms of computation power and time, it is highly dependent on human judgement.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma
|      **[B.2]** <span style="color: #FF0000">C</span> = cost
|
| **[C]** Performance of the applied manual grid search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves sigma=0.03000 and C=2
|      **[C.2]** Cross-Validation AUROC = 0.96629
|      **[C.3]** Test AUROC = 0.93780
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Transforming factor predictors
# as required by the nature of the model
##################################
# Creating a local object
# for the train and test sets
##################################
PMA_PreModelling_Train_SVM_R <- as.data.frame(lapply(PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in%
                                                                            c("Class")], 
                                                   function(x) as.numeric(as.character(x))))
PMA_PreModelling_Train_SVM_R$Class <- PMA_PreModelling_Train$Class
dim(PMA_PreModelling_Train_SVM_R)

PMA_PreModelling_Test_SVM_R <- as.data.frame(lapply(PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in%
                                                                          c("Class")],
                                                  function(x) as.numeric(as.character(x))))
PMA_PreModelling_Test_SVM_R$Class <- PMA_PreModelling_Test$Class
dim(PMA_PreModelling_Test_SVM_R)

##################################
# Creating consistent fold assignments
# for the 10-Fold Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train_SVM_R$Class,
                             k = 10,
                             returnTrain=TRUE)
KFold_Control_RandomSearch <- trainControl(method="cv",
                              index=KFold_Indices,
                              summaryFunction = twoClassSummary,
                              classProbs = TRUE,
                              search = "random")

KFold_Control_GridSearch <- trainControl(method="cv",
                              index=KFold_Indices,
                              summaryFunction = twoClassSummary,
                              classProbs = TRUE)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
SVM_R_Grid = expand.grid(sigma = c(0.030, 0.025, 0.020, 0.015, 0.010, 0.005), 
                       C = 2^(-2:9))

##################################
# Running the support vector machine (radial basis function kernel) model
# by setting the caret method to 'svmRadial'
##################################

##################################
# Using a manual grid search
##################################
set.seed(12345678)
SVM_R_Tune_GridSearch_Manual <- train(x = PMA_PreModelling_Train_SVM_R[,!names(PMA_PreModelling_Train_SVM_R) %in% c("Class")],
                 y = PMA_PreModelling_Train_SVM_R$Class,
                 method = "svmRadial",
                 tuneGrid = SVM_R_Grid,
                 metric = "ROC",                 
                 preProc = c("center", "scale"),
                 trControl = KFold_Control_GridSearch,
                 returnResamp = "all")

SVM_R_Tune_GridSearch_Manual$finalModel

SVM_R_Tune_GridSearch_Manual$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(SVM_R_Train_GridSearch_Manual_ROCCurveAUC <- SVM_R_Tune_GridSearch_Manual$results[SVM_R_Tune_GridSearch_Manual$results$C==SVM_R_Tune_GridSearch_Manual$bestTune$C &
                                                                                     SVM_R_Tune_GridSearch_Manual$results$sigma==SVM_R_Tune_GridSearch_Manual$bestTune$sigma,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
SVM_R_Test_GridSearch_Manual <- data.frame(SVM_R_Observed = PMA_PreModelling_Test_SVM_R$Class,
                      SVM_R_Predicted = predict(SVM_R_Tune_GridSearch_Manual,
                      PMA_PreModelling_Test_SVM_R[,!names(PMA_PreModelling_Test_SVM_R) %in% c("Class")],
                      type = "prob"))

SVM_R_Test_GridSearch_Manual

##################################
# Reporting the independent evaluation results
# for the test set
##################################
SVM_R_Test_GridSearch_Manual_ROC <- roc(response = SVM_R_Test_GridSearch_Manual$SVM_R_Observed,
             predictor = SVM_R_Test_GridSearch_Manual$SVM_R_Predicted.R,
             levels = rev(levels(SVM_R_Test_GridSearch_Manual$SVM_R_Observed)))

(SVM_R_Test_GridSearch_Manual_ROCCurveAUC <- auc(SVM_R_Test_GridSearch_Manual_ROC)[1])

```

</details>

###  1.5.2 Support Vector Machine - Radial Basis Function Kernel - Automated Grid Search (SVM_R_AGS)
|
| [Support Vector Machine](https://dl.acm.org/doi/10.1145/130385.130401) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| [Automated Grid Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the selection of the range and interval of model hyperparameter values to be assessed based on the model default settings. Utilizing all the possible combinations of the hyperparameter values, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. While this process is extensive in terms of computation power and time, it is the most efficient method as there is the least possibility of missing out on an optimal solution for a model.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma
|      **[B.2]** <span style="color: #FF0000">C</span> = cost
|
| **[C]** Performance of the applied automated grid search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves sigma=0.01012 and C=8
|      **[C.2]** Cross-Validation AUROC = 0.93514
|      **[C.3]** Test AUROC = 0.91148
| 
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|  

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Using an automated grid search
##################################
set.seed(12345678)
SVM_R_Tune_GridSearch_Auto <- train(x = PMA_PreModelling_Train_SVM_R[,!names(PMA_PreModelling_Train_SVM_R) %in% c("Class")],
                 y = PMA_PreModelling_Train_SVM_R$Class,
                 method = "svmRadial",
                 tuneLength = 12,
                 metric = "ROC",                 
                 preProc = c("center", "scale"),
                 trControl = KFold_Control_GridSearch,
                 returnResamp = "all")

SVM_R_Tune_GridSearch_Auto$finalModel

SVM_R_Tune_GridSearch_Auto$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(SVM_R_Train_GridSearch_Auto_ROCCurveAUC <- SVM_R_Tune_GridSearch_Auto$results[SVM_R_Tune_GridSearch_Auto$results$C==SVM_R_Tune_GridSearch_Auto$bestTune$C &
                                                                                 SVM_R_Tune_GridSearch_Auto$results$sigma==SVM_R_Tune_GridSearch_Auto$bestTune$sigma,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
SVM_R_Test_GridSearch_Auto <- data.frame(SVM_R_Observed = PMA_PreModelling_Test_SVM_R$Class,
                      SVM_R_Predicted = predict(SVM_R_Tune_GridSearch_Auto,
                      PMA_PreModelling_Test_SVM_R[,!names(PMA_PreModelling_Test_SVM_R) %in% c("Class")],
                      type = "prob"))

SVM_R_Test_GridSearch_Auto

##################################
# Reporting the independent evaluation results
# for the test set
##################################
SVM_R_Test_GridSearch_Auto_ROC <- roc(response = SVM_R_Test_GridSearch_Auto$SVM_R_Observed,
             predictor = SVM_R_Test_GridSearch_Auto$SVM_R_Predicted.R,
             levels = rev(levels(SVM_R_Test_GridSearch_Auto$SVM_R_Observed)))

(SVM_R_Test_GridSearch_Auto_ROCCurveAUC <- auc(SVM_R_Test_GridSearch_Auto_ROC)[1])
```

</details>

###  1.5.3 Support Vector Machine - Radial Basis Function Kernel - Automated Random Search (SVM_R_ARS)
|
| [Support Vector Machine](https://dl.acm.org/doi/10.1145/130385.130401) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| [Automated Random Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the selection of the range and interval of model hyperparameter values to be assessed based on the model default settings. Utilizing random combinations of the hyperparameter values, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. This process might be reasonably effective in terms of computation power and time, but may miss on a few combinations which could have been optimal ones.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma
|      **[B.2]** <span style="color: #FF0000">C</span> = cost
|
| **[C]** Performance of the applied automated random search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves sigma=0.02458 and C=183
|      **[C.2]** Cross-Validation AUROC = 0.96213
|      **[C.3]** Test AUROC = 0.93541
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Using an automated random search
##################################
set.seed(12345678)
SVM_R_Tune_RandomSearch_Auto <- train(x = PMA_PreModelling_Train_SVM_R[,!names(PMA_PreModelling_Train_SVM_R) %in% c("Class")],
                 y = PMA_PreModelling_Train_SVM_R$Class,
                 method = "svmRadial",
                 tuneLength = 30,
                 metric = "ROC",                 
                 preProc = c("center", "scale"),
                 trControl = KFold_Control_RandomSearch,
                 returnResamp = "all")

SVM_R_Tune_RandomSearch_Auto$finalModel

SVM_R_Tune_RandomSearch_Auto$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(SVM_R_Train_RandomSearch_Auto_ROCCurveAUC <- SVM_R_Tune_RandomSearch_Auto$results[SVM_R_Tune_RandomSearch_Auto$results$C==SVM_R_Tune_RandomSearch_Auto$bestTune$C &
                                                                                     SVM_R_Tune_RandomSearch_Auto$results$sigma==SVM_R_Tune_RandomSearch_Auto$bestTune$sigma,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
SVM_R_Test_RandomSearch_Auto <- data.frame(SVM_R_Observed = PMA_PreModelling_Test_SVM_R$Class,
                      SVM_R_Predicted = predict(SVM_R_Tune_RandomSearch_Auto,
                      PMA_PreModelling_Test_SVM_R[,!names(PMA_PreModelling_Test_SVM_R) %in% c("Class")],
                      type = "prob"))

SVM_R_Test_RandomSearch_Auto

##################################
# Reporting the independent evaluation results
# for the test set
##################################
SVM_R_Test_RandomSearch_Auto_ROC <- roc(response = SVM_R_Test_RandomSearch_Auto$SVM_R_Observed,
             predictor = SVM_R_Test_RandomSearch_Auto$SVM_R_Predicted.R,
             levels = rev(levels(SVM_R_Test_RandomSearch_Auto$SVM_R_Observed)))

(SVM_R_Test_RandomSearch_Auto_ROCCurveAUC <- auc(SVM_R_Test_RandomSearch_Auto_ROC)[1])

```

</details>

###  1.5.4 Regularized Discriminant Analysis - Manual Grid Search (RDA_MGS)
|
| [Regularized Discriminant Analysis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752) serves as a compromise between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA). LDA uses Bayes rule to determine the posterior probability that an observation belongs to a certain class. Due to the normal assumption of LDA, the posterior is defined by a multivariate Gaussian whose covariance matrix is assumed to be identical for all classes. New points are classified by computing the discriminant function (the enumerator of the posterior probability) and returning the class with the maximal value. The discriminant variables can be obtained through eigen-decompositions of the within-class and between-class variance. QDA is a variant of LDA in which an individual covariance matrix is estimated for every class of observations. QDA is particularly useful if there is prior knowledge that individual classes exhibit distinct covariances. RDA introduces a regularization parameter that shrinks the separate covariance matrices of QDA towards a common LDA matrix. The parameter can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches, with the optimal hyperparameter value determined through cross-validation. Since RDA is a regularization technique, it is particularly useful when there are many features that are potentially correlated.
|
| [Manual Grid Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the explicit selection of individual model hyperparameter values to be assessed as influenced by human domain background and experience. Based on these hyperparameter settings, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. While this process is practical in terms of computation power and time, it is highly dependent on human judgement.
|
| **[A]** The regularized discriminant analysis model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">gamma</span> = gamma
|      **[B.2]** <span style="color: #FF0000">lambda</span> = lambda
|
| **[C]** Performance of the applied manual grid search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves gamma=0.60 and lambda=0.20
|      **[C.2]** Cross-Validation AUROC = 0.91150
|      **[C.3]** Test AUROC = 0.87560
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Creating a local object
# for the train and test sets
##################################
PMA_PreModelling_Train_RDA <- PMA_PreModelling_Train
PMA_PreModelling_Test_RDA <- PMA_PreModelling_Test

##################################
# Creating consistent fold assignments
# for the 10-Fold Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train_RDA$Class,
                             k = 10,
                             returnTrain=TRUE)

KFold_Control_RandomSearch <- trainControl(method="cv",
                              index=KFold_Indices,
                              summaryFunction = twoClassSummary,
                              classProbs = TRUE,
                              search = "random")

KFold_Control_GridSearch <- trainControl(method="cv",
                              index=KFold_Indices,
                              summaryFunction = twoClassSummary,
                              classProbs = TRUE)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
RDA_Grid = expand.grid(gamma = seq(0,1,0.20), 
                       lambda = seq(0,1,0.20))

##################################
# Running the regularized discriminant analysis model
# by setting the caret method to 'rda'
##################################

##################################
# Using a manual grid search
##################################
set.seed(12345678)
RDA_Tune_GridSearch_Manual <- train(x = PMA_PreModelling_Train_RDA[,!names(PMA_PreModelling_Train_RDA) %in% c("Class")],
                 y = PMA_PreModelling_Train_RDA$Class,
                 method = "rda",
                 tuneGrid = RDA_Grid,
                 metric = "ROC",
                 trControl = KFold_Control_GridSearch,
                 returnResamp = "all")

RDA_Tune_GridSearch_Manual$finalModel

RDA_Tune_GridSearch_Manual$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(RDA_Train_GridSearch_Manual_ROCCurveAUC <- RDA_Tune_GridSearch_Manual$results[RDA_Tune_GridSearch_Manual$results$gamma==RDA_Tune_GridSearch_Manual$bestTune$gamma &
                                                                                 RDA_Tune_GridSearch_Manual$results$lambda==RDA_Tune_GridSearch_Manual$bestTune$lambda,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
RDA_Test_GridSearch_Manual <- data.frame(RDA_Observed = PMA_PreModelling_Test_RDA$Class,
                      RDA_Predicted = predict(RDA_Tune_GridSearch_Manual,
                      PMA_PreModelling_Test_RDA[,!names(PMA_PreModelling_Test_RDA) %in% c("Class")],
                      type = "prob"))

RDA_Test_GridSearch_Manual

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RDA_Test_GridSearch_Manual_ROC <- roc(response = RDA_Test_GridSearch_Manual$RDA_Observed,
             predictor = RDA_Test_GridSearch_Manual$RDA_Predicted.R,
             levels = rev(levels(RDA_Test_GridSearch_Manual$RDA_Observed)))

(RDA_Test_GridSearch_Manual_ROCCurveAUC <- auc(RDA_Test_GridSearch_Manual_ROC)[1])
```

</details>

###  1.5.5 Regularized Discriminant Analysis - Automated Grid Search (RDA_AGS)
|
| [Regularized Discriminant Analysis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752) serves as a compromise between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA). LDA uses Bayes rule to determine the posterior probability that an observation belongs to a certain class. Due to the normal assumption of LDA, the posterior is defined by a multivariate Gaussian whose covariance matrix is assumed to be identical for all classes. New points are classified by computing the discriminant function (the enumerator of the posterior probability) and returning the class with the maximal value. The discriminant variables can be obtained through eigen-decompositions of the within-class and between-class variance. QDA is a variant of LDA in which an individual covariance matrix is estimated for every class of observations. QDA is particularly useful if there is prior knowledge that individual classes exhibit distinct covariances. RDA introduces a regularization parameter that shrinks the separate covariance matrices of QDA towards a common LDA matrix. The parameter can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches, with the optimal hyperparameter value determined through cross-validation. Since RDA is a regularization technique, it is particularly useful when there are many features that are potentially correlated.
|
| [Automated Grid Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the selection of the range and interval of model hyperparameter values to be assessed based on the model default settings. Utilizing all the possible combinations of the hyperparameter values, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. While this process is extensive in terms of computation power and time, it is the most efficient method as there is the least possibility of missing out on an optimal solution for a model.
|
| **[A]** The regularized discriminant analysis model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">gamma</span> = gamma
|      **[B.2]** <span style="color: #FF0000">lambda</span> = lambda
|
| **[C]** Performance of the applied automated grid search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves gamma=0.57 and lambda=0.14
|      **[C.2]** Cross-Validation AUROC = 0.91014
|      **[C.3]** Test AUROC =  0.87081
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Using an automated grid search
##################################
set.seed(12345678)
RDA_Tune_GridSearch_Auto <- train(x = PMA_PreModelling_Train_RDA[,!names(PMA_PreModelling_Train_RDA) %in% c("Class")],
                 y = PMA_PreModelling_Train_RDA$Class,
                 method = "rda",
                 tuneLength = 8,
                 metric = "ROC",                 
                 trControl = KFold_Control_GridSearch,
                 returnResamp = "all")

RDA_Tune_GridSearch_Auto$finalModel

RDA_Tune_GridSearch_Auto$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(RDA_Train_GridSearch_Auto_ROCCurveAUC <- RDA_Tune_GridSearch_Auto$results[RDA_Tune_GridSearch_Auto$results$gamma==RDA_Tune_GridSearch_Auto$bestTune$gamma &
                                                                             RDA_Tune_GridSearch_Auto$results$lambda==RDA_Tune_GridSearch_Auto$bestTune$lambda,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
RDA_Test_GridSearch_Auto <- data.frame(RDA_Observed = PMA_PreModelling_Test_RDA$Class,
                      RDA_Predicted = predict(RDA_Tune_GridSearch_Auto,
                      PMA_PreModelling_Test_RDA[,!names(PMA_PreModelling_Test_RDA) %in% c("Class")],
                      type = "prob"))

RDA_Test_GridSearch_Auto

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RDA_Test_GridSearch_Auto_ROC <- roc(response = RDA_Test_GridSearch_Auto$RDA_Observed,
             predictor = RDA_Test_GridSearch_Auto$RDA_Predicted.R,
             levels = rev(levels(RDA_Test_GridSearch_Auto$RDA_Observed)))

(RDA_Test_GridSearch_Auto_ROCCurveAUC <- auc(RDA_Test_GridSearch_Auto_ROC)[1])
```

</details>

###  1.5.6 Regularized Discriminant Analysis - Automated Random Search (RDA_ARS)
|
| [Regularized Discriminant Analysis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752) serves as a compromise between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA). LDA uses Bayes rule to determine the posterior probability that an observation belongs to a certain class. Due to the normal assumption of LDA, the posterior is defined by a multivariate Gaussian whose covariance matrix is assumed to be identical for all classes. New points are classified by computing the discriminant function (the enumerator of the posterior probability) and returning the class with the maximal value. The discriminant variables can be obtained through eigen-decompositions of the within-class and between-class variance. QDA is a variant of LDA in which an individual covariance matrix is estimated for every class of observations. QDA is particularly useful if there is prior knowledge that individual classes exhibit distinct covariances. Regularized Discriminant Analysis (RDA) introduces a regularization parameter that shrinks the separate covariance matrices of QDA towards a common LDA matrix. The parameter can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches, with the optimal hyperparameter value determined through cross-validation. Since RDA is a regularization technique, it is particularly useful when there are many features that are potentially correlated.
|
| [Automated Random Search](https://link.springer.com/book/10.1007/978-981-19-5170-1) involves the selection of the range and interval of model hyperparameter values to be assessed based on the model default settings. Utilizing random combinations of the hyperparameter values, the model is trained with the performance measures evaluated. This process is repeated with another set of values for the same hyperparameters until optimal accuracy is achieved, or the model has attained minimal error. This process might be reasonably effective in terms of computation power and time, but may miss on a few combinations which could have been optimal ones.
|
| **[A]** The regularized discriminant analysis model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">gamma</span> = gamma
|      **[B.2]** <span style="color: #FF0000">lambda</span> = lambda
|
| **[C]** Performance of the applied automated random search method for hyperparameter tuning is summarized as follows :
|      **[C.1]** Final model configuration involves gamma=0.48 and lambda=0.09
|      **[C.2]** Cross-Validation AUROC = 0.90578
|      **[C.3]** Test AUROC = 0.861244
| 
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Using an automated random search
##################################
set.seed(12345678)
RDA_Tune_RandomSearch_Auto <- train(x = PMA_PreModelling_Train_RDA[,!names(PMA_PreModelling_Train_RDA) %in% c("Class")],
                 y = PMA_PreModelling_Train_RDA$Class,
                 method = "rda",
                 tuneLength = 36,
                 metric = "ROC",                 
                 trControl = KFold_Control_RandomSearch,
                 returnResamp = "all")

RDA_Tune_RandomSearch_Auto$finalModel

RDA_Tune_RandomSearch_Auto$results

##################################
# Reporting the cross-validation results
# for the train set
##################################
(RDA_Train_RandomSearch_Auto_ROCCurveAUC <- RDA_Tune_RandomSearch_Auto$results[RDA_Tune_RandomSearch_Auto$results$gamma==RDA_Tune_RandomSearch_Auto$bestTune$gamma &
                                                                                 RDA_Tune_RandomSearch_Auto$results$lambda==RDA_Tune_RandomSearch_Auto$bestTune$lambda,
                                                                               c("ROC")])

##################################
# Independently evaluating the model
# on the test set
##################################
RDA_Test_RandomSearch_Auto <- data.frame(RDA_Observed = PMA_PreModelling_Test_RDA$Class,
                      RDA_Predicted = predict(RDA_Tune_RandomSearch_Auto,
                      PMA_PreModelling_Test_RDA[,!names(PMA_PreModelling_Test_RDA) %in% c("Class")],
                      type = "prob"))

RDA_Test_RandomSearch_Auto

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RDA_Test_RandomSearch_Auto_ROC <- roc(response = RDA_Test_RandomSearch_Auto$RDA_Observed,
             predictor = RDA_Test_RandomSearch_Auto$RDA_Predicted.R,
             levels = rev(levels(RDA_Test_RandomSearch_Auto$RDA_Observed)))

(RDA_Test_RandomSearch_Auto_ROCCurveAUC <- auc(RDA_Test_RandomSearch_Auto_ROC)[1])

```

</details>

## 1.6 Consolidated Findings
|
| **[A]** The models which demonstrated the best and most consistent AUROC metrics are as follows:
|      **[A.1]** Hyperparameters tuned using **SVM_R_MGS: Support Vector Machine - Radial Basis Function Kernel - Manual Grid Search** (<mark style="background-color: #CCECFF">**kernlab**</mark> packages)
|             **[A.1.1]** Cross-Validation AUROC = 0.96629, Test AUROC = 0.93780 
|      **[A.2]** Hyperparameters tuned using **SVM_R_ARS: Support Vector Machine - Radial Basis Function Kernel - Automated Random Search** (<mark style="background-color: #CCECFF">**kernlab**</mark> packages)
|             **[A.2.1]** Cross-Validation AUROC = 0.96213, Test AUROC = 0.93541 
|
| **[B]** Between models, the **Support Vector Machine - Radial Basis Function Kernel** model demonstrated a more stable AUROC metrics driven by the narrower interquartile range and 95% confidence interval, as compared to the **Regularized Discriminant Analysis** model.
|
| **[C]** No statistically significant differences in the AUROC metrics were observed between models and hyperparameter tuning strategies, although the **SVM_R_MGS: Support Vector Machine - Radial Basis Function Kernel - Manual Grid Search** model demonstrated the lowest p-values (between 0.10 to 0.14) when specifically compared to the **RDA_MGS: Regularized Discriminant Analysis - Manual Grid Search**, **RDA_AGS: Regularized Discriminant Analysis - Automated Grid Search** and **RDA_ARS: Regularized Discriminant Analysis - Automated Random Search** models.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6, warning=FALSE, message=FALSE}

##################################
# Consolidating all results
# from the evaluated hyperparameter tuning methods
# for Support Vector Machine - Radial Basis Function Kernel (SVM_R)
##################################
SVM_R_Tune_GridSearch_Manual_ROC <- SVM_R_Tune_GridSearch_Manual$results
SVM_R_Tune_GridSearch_Auto_ROC   <- SVM_R_Tune_GridSearch_Auto$results
SVM_R_Tune_RandomSearch_Auto_ROC <- SVM_R_Tune_RandomSearch_Auto$results

SVM_R_Tune_All <- as.data.frame(rbind(SVM_R_Tune_GridSearch_Manual_ROC,
                        SVM_R_Tune_GridSearch_Auto_ROC,
                        SVM_R_Tune_RandomSearch_Auto_ROC))

SVM_R_Tune_All$Method <- c(rep("Manual Grid Search (MGS)",nrow(SVM_R_Tune_GridSearch_Manual_ROC)),
                           rep("Automated Grid Search (AGS)",nrow(SVM_R_Tune_GridSearch_Auto_ROC)),
                           rep("Automated Random Search (ARS)",nrow(SVM_R_Tune_RandomSearch_Auto_ROC)))

SVM_R_Tune_All$Method <- factor(SVM_R_Tune_All$Method,
                                levels=c("Manual Grid Search (MGS)",
                                         "Automated Grid Search (AGS)",
                                         "Automated Random Search (ARS)"))

##################################
# Plotting all results
# from the evaluated hyperparameter tuning methods
##################################
ggplot(SVM_R_Tune_All, aes(x=sigma, y=C, color=ROC, size= ROC)) +
        geom_point() +
        scale_color_gradient(low="blue", high="red") +
        theme_bw() +
        facet_grid(. ~ Method) +
        scale_x_continuous(name="Sigma", limits=c(0,0.03),breaks=seq(0,0.03,by=0.01)) +
        scale_y_continuous(name="C", limits=c(0,600),breaks=seq(0,600,by=100)) +
        theme(legend.position="top",
              plot.title=element_text(color="black",size=15,face="bold",hjust=0.50)) +
  ggtitle("Hyperparameter Tuning : Support Vector Machine - Radial Basis Function Kernel (SVM_R)")

##################################
# Consolidating all results
# from the evaluated hyperparameter tuning methods
# for Regularized Discriminant Analysis (RDA)
##################################
RDA_Tune_GridSearch_Manual_ROC <- RDA_Tune_GridSearch_Manual$results
RDA_Tune_GridSearch_Auto_ROC   <- RDA_Tune_GridSearch_Auto$results
RDA_Tune_RandomSearch_Auto_ROC <- RDA_Tune_RandomSearch_Auto$results

RDA_Tune_All <- as.data.frame(rbind(RDA_Tune_GridSearch_Manual_ROC,
                        RDA_Tune_GridSearch_Auto_ROC,
                        RDA_Tune_RandomSearch_Auto_ROC))

RDA_Tune_All$Method <- c(rep("Manual Grid Search (MGS)",nrow(RDA_Tune_GridSearch_Manual_ROC)),
                           rep("Automated Grid Search (AGS)",nrow(RDA_Tune_GridSearch_Auto_ROC)),
                           rep("Automated Random Search (ARS)",nrow(RDA_Tune_RandomSearch_Auto_ROC)))

RDA_Tune_All$Method <- factor(RDA_Tune_All$Method,
                                levels=c("Manual Grid Search (MGS)",
                                         "Automated Grid Search (AGS)",
                                         "Automated Random Search (ARS)"))

##################################
# Plotting all results
# from the evaluated hyperparameter tuning methods
##################################
ggplot(RDA_Tune_All, aes(x=gamma, y=lambda, color=ROC, size= ROC)) +
        geom_point() +
        scale_color_gradient(low="blue", high="red") +
        theme_bw() +
        facet_grid(. ~ Method) +
        scale_x_continuous(name="Gamma", limits=c(0,1),breaks=seq(0,1,by=0.10)) +
        scale_y_continuous(name="Lambda", limits=c(0,1),breaks=seq(0,1,by=0.10)) +
        theme(legend.position="top",
              plot.title=element_text(color="black",size=15,face="bold",hjust=0.50)) +
  ggtitle("Hyperparameter Tuning : Regularized Discriminant Analysis (RDA)")

##################################
# Consolidating all evaluation results
# for the train and test sets
# using the AUROC metric
##################################
Model <- c('SVM_R_MGS','SVM_R_AGS','SVM_R_ARS','RDA_MGS','RDA_AGS','RDA_ARS',
           'SVM_R_MGS','SVM_R_AGS','SVM_R_ARS','RDA_MGS','RDA_AGS','RDA_ARS')

Set <- c(rep('Cross-Validation',6),rep('Test',6))

ROCCurveAUC <- c(SVM_R_Train_GridSearch_Manual_ROCCurveAUC,
                 SVM_R_Train_GridSearch_Auto_ROCCurveAUC,
                 SVM_R_Train_RandomSearch_Auto_ROCCurveAUC,
                 RDA_Train_GridSearch_Manual_ROCCurveAUC,
                 RDA_Train_GridSearch_Auto_ROCCurveAUC,
                 RDA_Train_RandomSearch_Auto_ROCCurveAUC,
                 SVM_R_Test_GridSearch_Manual_ROCCurveAUC,
                 SVM_R_Test_GridSearch_Auto_ROCCurveAUC,
                 SVM_R_Test_RandomSearch_Auto_ROCCurveAUC,
                 RDA_Test_GridSearch_Manual_ROCCurveAUC,
                 RDA_Test_GridSearch_Auto_ROCCurveAUC,
                 RDA_Test_RandomSearch_Auto_ROCCurveAUC)

ROCCurveAUC_Summary <- as.data.frame(cbind(Model,Set,ROCCurveAUC))

ROCCurveAUC_Summary$ROCCurveAUC <- as.numeric(as.character(ROCCurveAUC_Summary$ROCCurveAUC))
ROCCurveAUC_Summary$Set <- factor(ROCCurveAUC_Summary$Set,
                                        levels = c("Cross-Validation",
                                                   "Test"))
ROCCurveAUC_Summary$Model <- factor(ROCCurveAUC_Summary$Model,
                                        levels = c('SVM_R_MGS',
                                                   'SVM_R_AGS',
                                                   'SVM_R_ARS',
                                                   'RDA_MGS',
                                                   'RDA_AGS',
                                                   'RDA_ARS'))

print(ROCCurveAUC_Summary, row.names=FALSE)

(ROCCurveAUC_Plot <- dotplot(Model ~ ROCCurveAUC,
                           data = ROCCurveAUC_Summary,
                           groups = Set,
                           main = "Classification Model Performance Comparison",
                           ylab = "Model",
                           xlab = "AUROC",
                           auto.key = list(adj = 1),
                           type=c("p", "h"),       
                           origin = 0,
                           alpha = 0.45,
                           pch = 16,
                           cex = 2))

##################################
# Consolidating the resampling results
# for the candidate models
##################################
(OverallResampling <- resamples(list(SVM_R_MGS = SVM_R_Tune_GridSearch_Manual,
                                    SVM_R_AGS = SVM_R_Tune_GridSearch_Auto,
                                    SVM_R_ARS = SVM_R_Tune_RandomSearch_Auto,
                                    RDA_MGS = RDA_Tune_GridSearch_Manual,
                                    RDA_AGS = RDA_Tune_GridSearch_Auto,
                                    RDA_ARS = RDA_Tune_RandomSearch_Auto)))
summary(OverallResampling)

##################################
# Exploring the resampling results
##################################
bwplot(OverallResampling,
       main = "Model Resampling Performance Comparison (Range)",
       ylab = "Model",
       pch=16,
       cex=2,
       layout=c(3,1))

dotplot(OverallResampling,
       main = "Model Resampling Performance Comparison (95% Confidence Interval)",
       ylab = "Model",
       pch=16,
       cex=2,
       layout=c(3,1))

splom(OverallResampling)

##################################
# Conducting an analysis
# of the performance differences
##################################
(ResamplingDifferences <- diff(OverallResampling))

summary(ResamplingDifferences)

bwplot(ResamplingDifferences,
       main = "Model Resampling Performance Comparison (Differences)",
       ylab = "Model",
       pch=16,
       cex=2,
       layout=c(3,1))

```

</details>
|
# **2. Summary** <a name="summary"></a>
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_12/images/Project12_Summary.png)
|
# **3. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Hyperparameter Tuning for Machine and Deep Learning with R](https://link.springer.com/book/10.1007/978-981-19-5170-1) by Eva Bartz, Thomas Bartz-Beielstein, Martin Zaefferer and Olaf Mersmann
| **[Book]** [Hyperparameter Optimization in Machine Learning](https://link.springer.com/book/10.1007/978-1-4842-6579-6) by Tanay Agrawal
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR2](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package  A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Hyperparameter Tuning for Machine Learning Explained](https://www.mygreatlearning.com/blog/hyperparameter-tuning-explained/) by Great Learning Team
| **[Article]** [Hyperparameter Tuning](https://www.geeksforgeeks.org/hyperparameter-tuning/) by Tarandeep Singh
| **[Article]** [Hyperparameter Optimization Techniques to Improve Your Machine Learning Model's Performance](https://www.freecodecamp.org/news/hyperparameter-optimization-techniques-machine-learning/) by Davis David
| **[Article]** [What Is Hyperparameter Tuning?](https://aws.amazon.com/what-is/hyperparameter-tuning/) by AWS Team
| **[Article]** [Hyperparameter Tuning in Python: a Complete Guide](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide#:~:text=Hyperparameter%20tuning%20%28or%20hyperparameter%20optimization%29%20is%20the%20process,running%20multiple%20trials%20in%20a%20single%20training%20process.) by Shahul ES and Aayush Bajaj
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Article]** [An Introduction to Naive Bayes Algorithm for Beginners](https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners) by Turing Team
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Nearest Shrunken Centroids With Python](https://machinelearningmastery.com/nearest-shrunken-centroids-with-python/) by Jason Brownlee
| **[Article]** [Discriminant Analysis Essentials in R](http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/) by Alboukadel Kassambara
| **[Article]** [Linear Discriminant Analysis, Explained](https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html) by Xiaozhou Yang
| **[Article]** [Flexible Discriminant Analysis](https://support.bccvl.org.au/support/solutions/articles/6000083206-flexible-discriminant-analysis#:~:text=Flexible%20Discriminant%20Analysis%20is%20a%20classification%20model%20based,adaptive%20regression%20splines%20to%20generate%20the%20discriminant%20surface.) by BCCVL Team
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [Random Forest](https://support.bccvl.org.au/support/solutions/articles/6000083217-random-forest) by BCCVL Team
| **[Article]** [Boosted Regression Tree](https://support.bccvl.org.au/support/solutions/articles/6000083202-boosted-regression-tree) by BCCVL Team
| **[Article]** [Artificial Neural Network](https://support.bccvl.org.au/support/solutions/articles/6000083200-artificial-neural-network) by BCCVL Team
| **[Article]** [Generalized Linear Model](https://support.bccvl.org.au/support/solutions/articles/6000083213-generalized-linear-model) by BCCVL Team
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [Generalized Boosting Model](https://support.bccvl.org.au/support/solutions/articles/6000083212-generalized-boosting-model) by BCCVL Team
| **[Article]** [C5.0: An Informal Tutorial](https://www.rulequest.com/see5-unix.html) by RuleQuest Team
| **[Article]** [What is Nearest Shrunken Centroid Classification?](https://www.tibshirani.su.domains/PAM/Rdist/howwork.html) by Rob Tibshirani
| **[Article]** [Regularized Discriminant Analysis](https://www.geeksforgeeks.org/regularized-discriminant-analysis/) by Geeks For Geeks Team
| **[Article]** [Linear, Quadratic, and Regularized Discriminant Analysis](https://www.datascienceblog.net/post/machine-learning/linear-discriminant-analysis/#:~:text=Linear%20discriminant%20analysis%20%28LDA%29%20is%20particularly%20popular%20because,%28RDA%29%20is%20a%20compromise%20between%20LDA%20and%20QDA.) by Data Science Blog Team
| **[Article]** [Regularized Discriminant Analysis](https://data-farmers.github.io/2019-06-25-Regularized-Discriminant-Analysis/) by Data Farmers
| **[Publication]** [A Training Algorithm for Optimal Margin Classifiers](https://dl.acm.org/doi/10.1145/130385.130401) by Bernhard Boser, Isabelle Guyon and Vladimir Vapnik (Proceedings of the Fifth Annual Workshop on Computational Learning Theory)
| **[Publication]** [Regularized Discriminant Analysis](https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752) by Jerome Friedman (Journal of the American Statistical Association)
| **[Publication]** [Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation](https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10477973) by Bradley Efron (Journal of the American Statistical Association)
| **[Publication]** [How Biased is the Apparent Error Rate of a Prediction Rule?](https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478291) by Bradley Efron (Journal of the American Statistical Association)
| **[Publication]** [Prediction Error Estimation: A Comparison of Resampling Methods](https://academic.oup.com/bioinformatics/article/21/15/3301/195433) by Annette Molinaro, Richard Simon and Ruth Pfeiffer (Bioinformatics)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|